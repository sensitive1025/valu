import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes

# 데이터셋 로드
diabetes = load_diabetes()
x = diabetes.data[:, 2]
y = diabetes.target


class SingleLayer:
  def __init__(self, learning_rate = 0.1, l1 = 0, l2 = 0):
    self.w = None
    self.b = None
    self.losses = []#손실함수의 결괏값을 저장할 리스트
    self.val_losses = []
    self.w_history = [] #가중치를 기록
    self.lr = learning_rate #학습률(가중치의 업데이트 양을 조절하는 하이퍼파라미터)
    self.l1 = l1
    self.l2 = l2
  
  def forpass(self, x):
    z = np.sum(x * self.w) + self.b #넘파이 배열에 사칙연산을 적용하면 배열의 요소끼리 계산함
    return z

  def backprop(self, x, err):
    w_grad = x * err
    b_grad = 1 * err
    return w_grad, b_grad

  def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
    self.w = np.ones(x.shape[1])#가중치를 1로 초기화
    self.b = 0
    self.w_history.append(self.w.copy())#가중치를 기록
    np.random.seed(42)#무작위로 시드 지정

    for i in range(epochs):
      loss = 0
      indexes = np.random.permutation(np.arange(len(x)))#인덱스를 랜덤으로 섞어서 인덱스 순서대로 샘플을 뽑음
      for j in indexes:
        z = self.forpass(x[j]) 
        a = self.activation(z) #활성화함수 적용
        err = -(y[j] - a) #오차 계산 
        w_grad, b_grad = self.backprop(x[j], err)#역방향 계산
        w_grad += self.l1 * np.sign(self.w) + self.l2 * self.w #L1, L2 규제 동시 수행
        self.w -= self.lr * w_grad #가중치 업데이트
        self.b -= b_grad #절편 업데이트
        self.w_history.append(self.w.copy()) #가중치 기록
        a = np.clip(a, 1e-10, 1-1e-10)#안전한 로그 계산을 위해 클리핑한 후 손실을 누적
        loss += -(y[j]*np.log(a)+(1-y[j])*np.log(1-a))#에포크마다 평균손실 저장 
      self.losses.append(loss/len(y) + self.reg_loss())#에포크마다 평균 손실 저장
      self.update_val_loss(x_val, y_val)#에포크마다 검증세트의 평균 손실 저장
      
  def activation(self, z):
    z = np.clip(z, -100, None) #안전한 np.exp 계산을 위해 (없으면 RuntimeWarning:overflow encountered in exp)
    a = 1 / (1 + np.exp(-z)) #시그모이드 
    return a

  def predict(self, x):
    z = [self.forpass(x_i) for x_i in x] #선형함수 적용
    return np.array(z) > 0 #계단함수 적용

  def score(self, x, y):
    return np.mean(self.predict(x) == y) 

  #로지스틱 손실함수 계산에 패널티 항 추가
  def reg_loss(self):
    return self.l1 * np.sum(np.abs(self.w)) + self.l2 / 2 * np.sum(self.w**2)
  
  #검증 세트의 손실 계산
  def update_val_loss(self, x_val, y_val):
    if x_val is None:
      return

    val_loss = 0
    for i in range(len(x_val)):
      z = self.forpass(x_val[i])
      a = self.activation(z)
      a = np.clip(a, 1e-10, 1-1e-10)
      val_loss += -(y_val[i]*np.log(a) + (1-y_val[i])*np.log(1-a))
    self.val_losses.append(val_loss/len(y_val) + self.reg_loss())
neuron = Neuron()
neuron.fit(x, y)
# 산점도 그리기
plt.scatter(x, y, color='blue', alpha=0.5)
plt.xlabel('BMI')
plt.ylabel('Progression')

pt1 = (-0.1, -0.1 * neuron.w + neuron.b)
pt2 = (0.15, 0.15 * neuron.w + neuron.b)
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])
# 경사 하강법으로 학습한 선형 회귀 선 그리기
plt.title('Diabetes Progression by Linear Regression (Gradient Descent)')
plt.show()
